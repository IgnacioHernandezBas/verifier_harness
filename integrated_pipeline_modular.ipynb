{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîß Modular Integrated Verification Pipeline\n",
        "\n",
        "**Complete patch verification with configurable analysis modules:**\n",
        "\n",
        "## Analysis Modules\n",
        "\n",
        "1. **Static Analysis** - Code quality (Pylint, Flake8, Radon, Mypy, Bandit)\n",
        "2. **Dynamic Fuzzing** - Change-aware property-based testing with real coverage\n",
        "3. **Supplementary Rules** - Targeted bug detection (9 focused verification rules)\n",
        "\n",
        "## Pipeline Flow\n",
        "\n",
        "```\n",
        "Load Patch ‚Üí Setup Repo ‚Üí [Static] ‚Üí [Fuzzing] ‚Üí [Rules] ‚Üí Final Verdict\n",
        "```\n",
        "\n",
        "Select which modules to enable in the **Configuration** cell below!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Configuration\n",
        "\n",
        "**Enable/disable analysis modules here:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# MODULAR CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "ANALYSIS_CONFIG = {\n",
        "    # Enable/disable analysis modules\n",
        "    'enable_static': True,      # Static code quality analysis\n",
        "    'enable_fuzzing': True,     # Dynamic fuzzing with coverage\n",
        "    'enable_rules': True,       # Supplementary verification rules\n",
        "    \n",
        "    # Thresholds\n",
        "    'static_threshold': 0.5,    # Min SQI score (0-1)\n",
        "    'coverage_threshold': 0.5,  # Min coverage of changed lines (0-1)\n",
        "    'rules_fail_on_high_severity': True,  # Reject on high-severity rule findings\n",
        "    \n",
        "    # Display options\n",
        "    'show_detailed_results': True,\n",
        "    'show_rule_findings': True,\n",
        "    'show_uncovered_lines': True,\n",
        "}\n",
        "\n",
        "# Repository filter\n",
        "REPO_FILTER = \"scikit-learn/scikit-learn\"\n",
        "\n",
        "print(\"‚úì Configuration loaded\")\n",
        "print(f\"  Static Analysis: {'‚úÖ' if ANALYSIS_CONFIG['enable_static'] else '‚ùå'}\")\n",
        "print(f\"  Dynamic Fuzzing: {'‚úÖ' if ANALYSIS_CONFIG['enable_fuzzing'] else '‚ùå'}\")\n",
        "print(f\"  Verification Rules: {'‚úÖ' if ANALYSIS_CONFIG['enable_rules'] else '‚ùå'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, subprocess, os\n",
        "from pathlib import Path\n",
        "import json, time, ast\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import importlib\n",
        "\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "sys.path.append(str(PROJECT_ROOT))\n",
        "\n",
        "# Dataset and patch handling\n",
        "from swebench_integration import DatasetLoader, PatchLoader\n",
        "\n",
        "# Dynamic container building system\n",
        "from swebench_singularity import Config, SingularityBuilder, DockerImageResolver\n",
        "\n",
        "# Analysis modules\n",
        "from verifier.dynamic_analyzers.patch_analyzer import PatchAnalyzer\n",
        "from verifier.dynamic_analyzers.test_generator import HypothesisTestGenerator\n",
        "from verifier.dynamic_analyzers.coverage_analyzer import CoverageAnalyzer\n",
        "from verifier.dynamic_analyzers.analyze_coverage_unified import analyze_coverage_unified\n",
        "from verifier.dynamic_analyzers import test_patch_singularity\n",
        "\n",
        "# Reload to get latest changes\n",
        "importlib.reload(test_patch_singularity)\n",
        "\n",
        "from verifier.dynamic_analyzers.test_patch_singularity import (\n",
        "    install_package_in_singularity, \n",
        "    run_tests_in_singularity,\n",
        "    install_hypothesis_in_singularity,\n",
        "    install_pytest_cov_in_singularity\n",
        ")\n",
        "\n",
        "# Static analysis\n",
        "import streamlit.modules.static_eval.static_modules.code_quality as code_quality\n",
        "import streamlit.modules.static_eval.static_modules.syntax_structure as syntax_structure\n",
        "from verifier.utils.diff_utils import parse_unified_diff, filter_paths_to_py\n",
        "\n",
        "# Verification rules\n",
        "from verifier.rules.runner import run_rules\n",
        "from verifier.rules import RULE_IDS\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "print(\"‚úì Imports complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Container Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set Apptainer/Singularity environment variables\n",
        "os.environ[\"APPTAINER_DOCKER_USERNAME\"] = \"nacheitor12\"\n",
        "os.environ[\"APPTAINER_DOCKER_PASSWORD\"] = \"wN/^4Me%,!5zz_q\"\n",
        "os.environ[\"SINGULARITY_DOCKER_USERNAME\"] = \"nacheitor12\"\n",
        "os.environ[\"SINGULARITY_DOCKER_PASSWORD\"] = \"wN/^4Me%,!5zz_q\"\n",
        "\n",
        "print(\"‚úì Docker credentials set\\n\")\n",
        "\n",
        "# Initialize configuration\n",
        "config = Config()\n",
        "config.set(\"singularity.cache_dir\", \"/fs/nexus-scratch/ihbas/.cache/swebench_singularity\")\n",
        "config.set(\"singularity.tmp_dir\", \"/fs/nexus-scratch/ihbas/.tmp/singularity_build\")\n",
        "config.set(\"singularity.cache_internal_dir\", \"/fs/nexus-scratch/ihbas/.singularity/cache\")\n",
        "config.set(\"singularity.build_timeout\", 1800)\n",
        "config.set(\"docker.max_retries\", 3)\n",
        "config.set(\"docker.image_patterns\", [\n",
        "    \"swebench/sweb.eval.x86_64.{repo}_1776_{repo}-{version}:latest\",\n",
        "])\n",
        "\n",
        "builder = SingularityBuilder(config)\n",
        "resolver = DockerImageResolver(config)\n",
        "\n",
        "print(\"‚úì Configuration initialized\")\n",
        "print(f\"  Singularity: {builder.check_singularity_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# PATCH LOADING & SETUP\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 1: Load Patch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loader = DatasetLoader(\"princeton-nlp/SWE-bench_Verified\", hf_mode=True, split=\"test\")\n",
        "sample = next(loader.iter_samples(limit=1, filter_repo=REPO_FILTER), None)\n",
        "\n",
        "if sample:\n",
        "    instance_id = sample.get('metadata', {}).get('instance_id', 'unknown')\n",
        "    print(f\"‚úì {instance_id}\")\n",
        "    print(f\"  Repo: {sample['repo']}\")\n",
        "    print(f\"\\nPatch preview:\\n{sample['patch'][:400]}...\")\n",
        "else:\n",
        "    raise Exception(f\"No sample found for {REPO_FILTER}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 2: Setup Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "patcher = PatchLoader(sample=sample, repos_root=\"./repos_temp\")\n",
        "repo_path = patcher.clone_repository()\n",
        "patch_result = patcher.apply_patch()\n",
        "\n",
        "print(f\"‚úì Repo: {repo_path}\")\n",
        "print(f\"‚úì Patch: {'Applied' if patch_result['applied'] else 'FAILED'}\")\n",
        "\n",
        "# Apply test patch\n",
        "test_patch = sample.get('metadata', {}).get('test_patch', '')\n",
        "if test_patch and test_patch.strip():\n",
        "    print(\"üìù Applying test_patch...\")\n",
        "    try:\n",
        "        test_patch_result = patcher.apply_additional_patch(test_patch)\n",
        "        print(f\"‚úì Test patch applied: {test_patch_result.get('log', 'success')}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Test patch application failed: {e}\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è  No test_patch in metadata\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 3: Build Container"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"üê≥ Building container for {instance_id}...\\n\")\n",
        "\n",
        "docker_image = resolver.find_available_image(instance_id, check_existence=False)\n",
        "if docker_image:\n",
        "    print(f\"  Docker image: {docker_image.full_name}\\n\")\n",
        "\n",
        "build_result = builder.build_instance(\n",
        "    instance_id=instance_id,\n",
        "    force_rebuild=False,\n",
        "    check_docker_exists=False\n",
        ")\n",
        "\n",
        "if build_result.success:\n",
        "    CONTAINER_IMAGE_PATH = build_result.sif_path\n",
        "    cache_status = \"(from cache)\" if build_result.from_cache else \"(newly built)\"\n",
        "    print(f\"‚úì Container ready {cache_status}\")\n",
        "    print(f\"  Path: {CONTAINER_IMAGE_PATH}\")\n",
        "else:\n",
        "    raise Exception(f\"Container build failed: {build_result.error_message}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 4: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üì¶ Installing dependencies...\\n\")\n",
        "\n",
        "install_result = install_package_in_singularity(\n",
        "    repo_path=Path(repo_path),\n",
        "    image_path=str(CONTAINER_IMAGE_PATH)\n",
        ")\n",
        "\n",
        "if install_result.get(\"installed\"):\n",
        "    print(\"‚úì Dependencies installed\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è  Using PYTHONPATH mode\")\n",
        "\n",
        "# Install pytest-cov for coverage\n",
        "if ANALYSIS_CONFIG['enable_fuzzing']:\n",
        "    print(\"\\nüì¶ Installing pytest-cov...\")\n",
        "    pytest_cov_result = install_pytest_cov_in_singularity(\n",
        "        repo_path=Path(repo_path),\n",
        "        image_path=str(CONTAINER_IMAGE_PATH)\n",
        "    )\n",
        "    if pytest_cov_result['installed']:\n",
        "        print(\"‚úì pytest-cov ready\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  pytest-cov installation failed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# MODULE 1: STATIC ANALYSIS\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if ANALYSIS_CONFIG['enable_static']:\n",
        "    print(\"=\"*80)\n",
        "    print(\"MODULE 1: STATIC ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    static_config = {\n",
        "        'checks': {'pylint': True, 'flake8': True, 'radon': True, 'mypy': True, 'bandit': True},\n",
        "        'weights': {'pylint': 0.5, 'flake8': 0.15, 'radon': 0.25, 'mypy': 0.05, 'bandit': 0.05}\n",
        "    }\n",
        "    \n",
        "    print(\"\\nüîç Running static analysis...\")\n",
        "    cq_results = code_quality.analyze(str(repo_path), sample['patch'], static_config)\n",
        "    ss_results = syntax_structure.run_syntax_structure_analysis(str(repo_path), sample['patch'])\n",
        "    \n",
        "    sqi_data = cq_results.get('sqi', {})\n",
        "    sqi_score = sqi_data.get('SQI', 0) / 100.0\n",
        "    \n",
        "    print(f\"\\n‚úì SQI: {sqi_data.get('SQI', 0)}/100 ({sqi_data.get('classification', 'Unknown')})\")\n",
        "    \n",
        "    if ANALYSIS_CONFIG['show_detailed_results']:\n",
        "        print(f\"\\n  Details:\")\n",
        "        for tool, data in cq_results.get('tools', {}).items():\n",
        "            score = data.get('score', 0)\n",
        "            print(f\"    {tool}: {score:.1f}/100\")\n",
        "    \n",
        "    # Check threshold\n",
        "    if sqi_score < ANALYSIS_CONFIG['static_threshold']:\n",
        "        print(f\"\\n‚ùå FAILED: SQI below threshold ({sqi_score:.2f} < {ANALYSIS_CONFIG['static_threshold']})\")\n",
        "        static_passed = False\n",
        "    else:\n",
        "        print(f\"\\n‚úÖ PASSED: SQI meets threshold\")\n",
        "        static_passed = True\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è  Static analysis skipped (disabled)\")\n",
        "    sqi_score = 0.0\n",
        "    sqi_data = {}\n",
        "    static_passed = True  # Don't fail if disabled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# MODULE 2: DYNAMIC FUZZING\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Analyze Patch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if ANALYSIS_CONFIG['enable_fuzzing']:\n",
        "    print(\"=\"*80)\n",
        "    print(\"MODULE 2: DYNAMIC FUZZING\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    print(\"\\nüîç Analyzing patch...\")\n",
        "    \n",
        "    patch_analyzer = PatchAnalyzer()\n",
        "    parsed_diff = parse_unified_diff(sample['patch'])\n",
        "    modified_files = filter_paths_to_py(list(parsed_diff.keys()))\n",
        "    \n",
        "    if modified_files:\n",
        "        first_file_path = modified_files[0]\n",
        "        first_file = Path(repo_path) / first_file_path\n",
        "        patched_code = first_file.read_text(encoding='utf-8')\n",
        "        \n",
        "        patch_analysis = patch_analyzer.parse_patch(sample['patch'], patched_code, file_path=first_file_path)\n",
        "        \n",
        "        print(f\"\\n‚úì Analysis complete:\")\n",
        "        print(f\"  Files: {len(modified_files)}\")\n",
        "        print(f\"  Module: {patch_analysis.module_path}\")\n",
        "        print(f\"  Functions: {patch_analysis.changed_functions}\")\n",
        "        if patch_analysis.class_context:\n",
        "            print(f\"  Classes: {list(patch_analysis.class_context.values())}\")\n",
        "        print(f\"  Changed lines: {len(patch_analysis.all_changed_lines)}\")\n",
        "        \n",
        "        coverage_source = patch_analysis.module_path.split('.')[0]\n",
        "    else:\n",
        "        patch_analysis = None\n",
        "        patched_code = None\n",
        "        coverage_source = None\n",
        "        print(\"‚ö†Ô∏è No Python files modified\")\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è  Dynamic fuzzing skipped (disabled)\")\n",
        "    patch_analysis = None\n",
        "    patched_code = None\n",
        "    coverage_source = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Run Baseline Tests (with coverage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if ANALYSIS_CONFIG['enable_fuzzing'] and patch_analysis:\n",
        "    print(\"\\nüß™ Running baseline tests...\\n\")\n",
        "    \n",
        "    fail_to_pass = sample.get('metadata', {}).get('FAIL_TO_PASS', '[]')\n",
        "    pass_to_pass = sample.get('metadata', {}).get('PASS_TO_PASS', '[]')\n",
        "    \n",
        "    try:\n",
        "        f2p = ast.literal_eval(fail_to_pass) if isinstance(fail_to_pass, str) else fail_to_pass\n",
        "        p2p = ast.literal_eval(pass_to_pass) if isinstance(pass_to_pass, str) else pass_to_pass\n",
        "    except:\n",
        "        f2p, p2p = [], []\n",
        "    \n",
        "    all_tests = f2p + p2p\n",
        "    \n",
        "    test_result = run_tests_in_singularity(\n",
        "        repo_path=Path(repo_path),\n",
        "        tests=all_tests,\n",
        "        image_path=str(CONTAINER_IMAGE_PATH),\n",
        "        collect_coverage=True,\n",
        "        coverage_source=coverage_source,\n",
        "    )\n",
        "    \n",
        "    existing_tests_pass = (test_result['returncode'] == 0)\n",
        "    print(f\"\\n{'‚úì' if existing_tests_pass else '‚ö†Ô∏è'} Tests {'passed' if existing_tests_pass else 'had issues'}\")\n",
        "    \n",
        "    # Analyze baseline coverage\n",
        "    if 'coverage_file' in test_result and test_result['coverage_file']:\n",
        "        baseline_cov_file = Path(test_result['coverage_file'])\n",
        "        \n",
        "        if baseline_cov_file.exists():\n",
        "            baseline_coverage_data = json.loads(baseline_cov_file.read_text())\n",
        "            analyzer = CoverageAnalyzer()\n",
        "            \n",
        "            baseline_analysis = analyze_coverage_unified(\n",
        "                coverage_data=baseline_coverage_data,\n",
        "                patch_analysis=patch_analysis,\n",
        "                analyzer=analyzer,\n",
        "                label=\"BASELINE\"\n",
        "            )\n",
        "            \n",
        "            if baseline_analysis:\n",
        "                baseline_coverage = baseline_analysis['line_coverage']\n",
        "                baseline_covered_lines = baseline_analysis['covered_lines']\n",
        "                baseline_branch_coverage = baseline_analysis['branch_coverage']\n",
        "            else:\n",
        "                baseline_coverage = 0.0\n",
        "                baseline_covered_lines = set()\n",
        "                baseline_branch_coverage = 0.0\n",
        "        else:\n",
        "            baseline_coverage = 0.0\n",
        "            baseline_covered_lines = set()\n",
        "            baseline_branch_coverage = 0.0\n",
        "    else:\n",
        "        baseline_coverage = 0.0\n",
        "        baseline_covered_lines = set()\n",
        "        baseline_branch_coverage = 0.0\n",
        "else:\n",
        "    existing_tests_pass = True\n",
        "    baseline_coverage = 0.0\n",
        "    baseline_covered_lines = set()\n",
        "    baseline_branch_coverage = 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Generate & Run Fuzzing Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if ANALYSIS_CONFIG['enable_fuzzing'] and patch_analysis and patch_analysis.changed_functions:\n",
        "    print(\"\\nüß¨ Generating fuzzing tests...\\n\")\n",
        "    \n",
        "    test_generator = HypothesisTestGenerator(repo_path=Path(repo_path))\n",
        "    test_code = test_generator.generate_tests(patch_analysis, patched_code)\n",
        "    test_count = test_code.count('def test_')\n",
        "    \n",
        "    test_file = Path(repo_path) / \"test_fuzzing_generated.py\"\n",
        "    test_file.write_text(test_code, encoding='utf-8')\n",
        "    \n",
        "    print(f\"‚úì Generated {test_count} tests\")\n",
        "    \n",
        "    # Install hypothesis\n",
        "    print(\"\\nüì¶ Installing hypothesis...\")\n",
        "    hypothesis_install = install_hypothesis_in_singularity(\n",
        "        repo_path=Path(repo_path),\n",
        "        image_path=str(CONTAINER_IMAGE_PATH)\n",
        "    )\n",
        "    \n",
        "    # Preserve baseline coverage\n",
        "    baseline_cov_json = Path(repo_path) / \".coverage.json\"\n",
        "    if baseline_cov_json.exists():\n",
        "        baseline_cov_json.rename(Path(repo_path) / \".coverage.baseline.json\")\n",
        "    \n",
        "    baseline_cov_db = Path(repo_path) / \".coverage\"\n",
        "    if baseline_cov_db.exists():\n",
        "        baseline_cov_db.unlink()\n",
        "    \n",
        "    print(\"\\nüê≥ Executing fuzzing tests...\\n\")\n",
        "    \n",
        "    fuzzing_result = run_tests_in_singularity(\n",
        "        repo_path=Path(repo_path),\n",
        "        tests=[\"test_fuzzing_generated.py\"],\n",
        "        image_path=str(CONTAINER_IMAGE_PATH),\n",
        "        extra_env={\"HYPOTHESIS_MAX_EXAMPLES\": \"50\"},\n",
        "        collect_coverage=True,\n",
        "        coverage_source=coverage_source,\n",
        "    )\n",
        "    \n",
        "    fuzzing_success = (fuzzing_result['returncode'] == 0)\n",
        "    print(f\"\\n{'‚úì' if fuzzing_success else '‚ö†Ô∏è'} Fuzzing tests {'passed' if fuzzing_success else 'had issues'}\")\n",
        "    \n",
        "    # Analyze fuzzing coverage\n",
        "    if 'coverage_file' in fuzzing_result and fuzzing_result['coverage_file']:\n",
        "        fuzzing_cov_file = Path(fuzzing_result['coverage_file'])\n",
        "        \n",
        "        if fuzzing_cov_file.exists():\n",
        "            fuzzing_coverage_data = json.loads(fuzzing_cov_file.read_text())\n",
        "            \n",
        "            fuzzing_analysis = analyze_coverage_unified(\n",
        "                coverage_data=fuzzing_coverage_data,\n",
        "                patch_analysis=patch_analysis,\n",
        "                analyzer=CoverageAnalyzer(),\n",
        "                label=\"FUZZING\"\n",
        "            )\n",
        "            \n",
        "            if fuzzing_analysis:\n",
        "                fuzzing_only_covered = fuzzing_analysis['covered_lines']\n",
        "                fuzzing_branch_coverage = fuzzing_analysis['branch_coverage']\n",
        "                \n",
        "                # Combine baseline + fuzzing\n",
        "                combined_covered_lines = baseline_covered_lines | fuzzing_only_covered\n",
        "                all_changed = set(patch_analysis.all_changed_lines)\n",
        "                combined_uncovered_lines = sorted(list(all_changed - combined_covered_lines))\n",
        "                combined_coverage = len(combined_covered_lines) / len(all_changed) if all_changed else 0.0\n",
        "                combined_branch_coverage = max(baseline_branch_coverage, fuzzing_branch_coverage)\n",
        "                \n",
        "                print(f\"\\nüìä COMBINED COVERAGE:\")\n",
        "                print(f\"   Line: {combined_coverage*100:.1f}%\")\n",
        "                print(f\"   Covered: {len(combined_covered_lines)}/{len(all_changed)} lines\")\n",
        "                if ANALYSIS_CONFIG['show_uncovered_lines'] and combined_uncovered_lines:\n",
        "                    print(f\"   Uncovered: {combined_uncovered_lines[:10]}...\")\n",
        "            else:\n",
        "                combined_coverage = baseline_coverage\n",
        "                combined_covered_lines = baseline_covered_lines\n",
        "                combined_uncovered_lines = sorted(list(set(patch_analysis.all_changed_lines) - baseline_covered_lines))\n",
        "                combined_branch_coverage = baseline_branch_coverage\n",
        "        else:\n",
        "            combined_coverage = baseline_coverage\n",
        "            combined_covered_lines = baseline_covered_lines\n",
        "            combined_uncovered_lines = sorted(list(set(patch_analysis.all_changed_lines) - baseline_covered_lines))\n",
        "            combined_branch_coverage = baseline_branch_coverage\n",
        "    else:\n",
        "        combined_coverage = baseline_coverage\n",
        "        combined_covered_lines = baseline_covered_lines\n",
        "        combined_uncovered_lines = sorted(list(set(patch_analysis.all_changed_lines) - baseline_covered_lines))\n",
        "        combined_branch_coverage = baseline_branch_coverage\n",
        "    \n",
        "    # Check threshold\n",
        "    if combined_coverage < ANALYSIS_CONFIG['coverage_threshold']:\n",
        "        print(f\"\\n‚ö†Ô∏è  WARNING: Coverage below threshold ({combined_coverage:.2f} < {ANALYSIS_CONFIG['coverage_threshold']})\")\n",
        "        fuzzing_passed = False\n",
        "    else:\n",
        "        print(f\"\\n‚úÖ Coverage meets threshold\")\n",
        "        fuzzing_passed = True\n",
        "        \n",
        "else:\n",
        "    fuzzing_success = True\n",
        "    combined_coverage = baseline_coverage\n",
        "    combined_covered_lines = baseline_covered_lines\n",
        "    combined_uncovered_lines = []\n",
        "    combined_branch_coverage = 0.0\n",
        "    fuzzing_passed = True\n",
        "    test_count = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# MODULE 3: SUPPLEMENTARY RULES\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if ANALYSIS_CONFIG['enable_rules']:\n",
        "    print(\"=\"*80)\n",
        "    print(\"MODULE 3: SUPPLEMENTARY VERIFICATION RULES\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    print(f\"\\nüîç Running {len(RULE_IDS)} verification rules...\\n\")\n",
        "    \n",
        "    try:\n",
        "        rules_results = run_rules(\n",
        "            rule_ids=RULE_IDS,\n",
        "            repo_path=str(repo_path),\n",
        "            patch_str=sample['patch']\n",
        "        )\n",
        "        \n",
        "        # Aggregate findings\n",
        "        all_findings = []\n",
        "        failed_rules = []\n",
        "        \n",
        "        for result in rules_results:\n",
        "            if result.status == 'failed':\n",
        "                failed_rules.append(result.name)\n",
        "                all_findings.extend(result.findings)\n",
        "        \n",
        "        print(f\"‚úì Rules execution complete\")\n",
        "        print(f\"  Total rules: {len(rules_results)}\")\n",
        "        print(f\"  Passed: {len(rules_results) - len(failed_rules)}\")\n",
        "        print(f\"  Failed: {len(failed_rules)}\")\n",
        "        print(f\"  Findings: {len(all_findings)}\")\n",
        "        \n",
        "        # Count severity\n",
        "        high_severity_count = sum(1 for f in all_findings if f.get('severity') == 'high')\n",
        "        \n",
        "        if ANALYSIS_CONFIG['show_rule_findings'] and all_findings:\n",
        "            print(f\"\\n  Findings by severity:\")\n",
        "            print(f\"    High: {high_severity_count}\")\n",
        "            print(f\"    Medium: {sum(1 for f in all_findings if f.get('severity') == 'medium')}\")\n",
        "            print(f\"    Low: {sum(1 for f in all_findings if f.get('severity') == 'low')}\")\n",
        "            \n",
        "            print(f\"\\n  Sample findings:\")\n",
        "            for finding in all_findings[:5]:\n",
        "                severity_icon = 'üî¥' if finding['severity'] == 'high' else 'üü°' if finding['severity'] == 'medium' else 'üü¢'\n",
        "                desc = finding['description'][:80]\n",
        "                loc = finding.get('location', 'N/A')\n",
        "                print(f\"    {severity_icon} {desc}... ({loc})\")\n",
        "            \n",
        "            if len(all_findings) > 5:\n",
        "                print(f\"    ... and {len(all_findings) - 5} more\")\n",
        "        \n",
        "        # Check if failed\n",
        "        if high_severity_count > 0 and ANALYSIS_CONFIG['rules_fail_on_high_severity']:\n",
        "            print(f\"\\n‚ùå FAILED: {high_severity_count} high-severity finding(s)\")\n",
        "            rules_passed = False\n",
        "        elif len(failed_rules) > 0:\n",
        "            print(f\"\\n‚ö†Ô∏è  WARNING: {len(failed_rules)} rule(s) found issues\")\n",
        "            rules_passed = True  # Warning, not failure\n",
        "        else:\n",
        "            print(f\"\\n‚úÖ PASSED: All rules passed\")\n",
        "            rules_passed = True\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ö†Ô∏è Rules execution error: {e}\")\n",
        "        failed_rules = []\n",
        "        all_findings = []\n",
        "        high_severity_count = 0\n",
        "        rules_passed = True  # Don't fail on error\n",
        "        \n",
        "else:\n",
        "    print(\"‚è≠Ô∏è  Supplementary rules skipped (disabled)\")\n",
        "    failed_rules = []\n",
        "    all_findings = []\n",
        "    high_severity_count = 0\n",
        "    rules_passed = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# FINAL VERDICT\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL VERDICT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Calculate overall score\n",
        "weights = {\n",
        "    'static': 30 if ANALYSIS_CONFIG['enable_static'] else 0,\n",
        "    'tests': 40,\n",
        "    'fuzzing': 15 if ANALYSIS_CONFIG['enable_fuzzing'] else 0,\n",
        "    'coverage': 10 if ANALYSIS_CONFIG['enable_fuzzing'] else 0,\n",
        "    'branch': 5 if ANALYSIS_CONFIG['enable_fuzzing'] else 0,\n",
        "}\n",
        "\n",
        "# Normalize weights to sum to 100\n",
        "total_weight = sum(weights.values())\n",
        "for key in weights:\n",
        "    weights[key] = (weights[key] / total_weight) * 100\n",
        "\n",
        "overall_score = (\n",
        "    (sqi_score * 100 if ANALYSIS_CONFIG['enable_static'] else 0) * (weights['static'] / 100) +\n",
        "    (100 if existing_tests_pass else 0) * (weights['tests'] / 100) +\n",
        "    (100 if fuzzing_success else 0) * (weights['fuzzing'] / 100) +\n",
        "    (combined_coverage * 100 if ANALYSIS_CONFIG['enable_fuzzing'] else 0) * (weights['coverage'] / 100) +\n",
        "    (combined_branch_coverage * 100 if ANALYSIS_CONFIG['enable_fuzzing'] else 0) * (weights['branch'] / 100)\n",
        ")\n",
        "\n",
        "# Determine verdict\n",
        "failed_checks = []\n",
        "if not static_passed:\n",
        "    failed_checks.append(\"Static analysis\")\n",
        "if not existing_tests_pass:\n",
        "    failed_checks.append(\"Existing tests\")\n",
        "if not fuzzing_success:\n",
        "    failed_checks.append(\"Fuzzing tests\")\n",
        "if not fuzzing_passed:\n",
        "    failed_checks.append(\"Coverage threshold\")\n",
        "if not rules_passed:\n",
        "    failed_checks.append(\"Verification rules\")\n",
        "\n",
        "if failed_checks:\n",
        "    if any(c in failed_checks for c in [\"Static analysis\", \"Existing tests\", \"Verification rules\"]):\n",
        "        verdict = \"‚ùå REJECT\"\n",
        "        reason = f\"Failed: {', '.join(failed_checks)}\"\n",
        "    else:\n",
        "        verdict = \"‚ö†Ô∏è  WARNING\"\n",
        "        reason = f\"Warnings: {', '.join(failed_checks)}\"\n",
        "else:\n",
        "    if overall_score >= 80:\n",
        "        verdict = \"‚úÖ EXCELLENT\"\n",
        "        reason = \"All checks passed with high score\"\n",
        "    elif overall_score >= 60:\n",
        "        verdict = \"‚úì GOOD\"\n",
        "        reason = \"All checks passed\"\n",
        "    else:\n",
        "        verdict = \"‚ö†Ô∏è  FAIR\"\n",
        "        reason = \"Passed but with low overall score\"\n",
        "\n",
        "print(f\"\\nInstance: {instance_id}\")\n",
        "print(f\"Overall Score: {overall_score:.1f}/100\")\n",
        "print(f\"Verdict: {verdict}\")\n",
        "print(f\"Reason: {reason}\\n\")\n",
        "\n",
        "print(\"Component Results:\")\n",
        "if ANALYSIS_CONFIG['enable_static']:\n",
        "    print(f\"  Static Analysis: {sqi_score*100:.1f}/100 {'‚úÖ' if static_passed else '‚ùå'}\")\n",
        "print(f\"  Existing Tests: {'PASS ‚úÖ' if existing_tests_pass else 'FAIL ‚ùå'}\")\n",
        "if ANALYSIS_CONFIG['enable_fuzzing']:\n",
        "    print(f\"  Fuzzing Tests: {'PASS ‚úÖ' if fuzzing_success else 'FAIL ‚ùå'} ({test_count} generated)\")\n",
        "    print(f\"  Coverage: {combined_coverage*100:.1f}% {'‚úÖ' if fuzzing_passed else '‚ö†Ô∏è'}\")\n",
        "if ANALYSIS_CONFIG['enable_rules']:\n",
        "    print(f\"  Verification Rules: {len(rules_results) - len(failed_rules)}/{len(rules_results)} passed {'‚úÖ' if rules_passed else '‚ùå'}\")\n",
        "    if high_severity_count > 0:\n",
        "        print(f\"    ‚ö†Ô∏è  {high_severity_count} high-severity finding(s)\")\n",
        "\n",
        "# Save results\n",
        "results = {\n",
        "    'instance_id': instance_id,\n",
        "    'overall_score': overall_score,\n",
        "    'verdict': verdict,\n",
        "    'reason': reason,\n",
        "    'enabled_modules': {\n",
        "        'static': ANALYSIS_CONFIG['enable_static'],\n",
        "        'fuzzing': ANALYSIS_CONFIG['enable_fuzzing'],\n",
        "        'rules': ANALYSIS_CONFIG['enable_rules'],\n",
        "    },\n",
        "    'static': {\n",
        "        'sqi_score': sqi_score * 100,\n",
        "        'passed': static_passed\n",
        "    } if ANALYSIS_CONFIG['enable_static'] else None,\n",
        "    'fuzzing': {\n",
        "        'tests_passed': existing_tests_pass,\n",
        "        'fuzzing_passed': fuzzing_success,\n",
        "        'tests_generated': test_count,\n",
        "        'combined_coverage': combined_coverage * 100,\n",
        "        'baseline_coverage': baseline_coverage * 100,\n",
        "        'improvement': (combined_coverage - baseline_coverage) * 100,\n",
        "        'passed': fuzzing_passed\n",
        "    } if ANALYSIS_CONFIG['enable_fuzzing'] else None,\n",
        "    'rules': {\n",
        "        'total_rules': len(rules_results),\n",
        "        'failed_rules': len(failed_rules),\n",
        "        'findings_count': len(all_findings),\n",
        "        'high_severity_count': high_severity_count,\n",
        "        'passed': rules_passed\n",
        "    } if ANALYSIS_CONFIG['enable_rules'] else None,\n",
        "}\n",
        "\n",
        "with open('integrated_pipeline_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"\\nüíæ Results saved to integrated_pipeline_results.json\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create visualization\n",
        "modules_enabled = sum([ANALYSIS_CONFIG['enable_static'], ANALYSIS_CONFIG['enable_fuzzing'], ANALYSIS_CONFIG['enable_rules']])\n",
        "\n",
        "if modules_enabled > 1:\n",
        "    fig, axes = plt.subplots(1, min(3, modules_enabled), figsize=(6*min(3, modules_enabled), 5))\n",
        "    if modules_enabled == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    ax_idx = 0\n",
        "    \n",
        "    # Static analysis\n",
        "    if ANALYSIS_CONFIG['enable_static'] and ax_idx < len(axes):\n",
        "        ax = axes[ax_idx]\n",
        "        color = '#2ecc71' if static_passed else '#e74c3c'\n",
        "        ax.bar(['SQI'], [sqi_score * 100], color=color, alpha=0.7)\n",
        "        ax.set_ylim(0, 100)\n",
        "        ax.set_ylabel('Score')\n",
        "        ax.set_title('Static Analysis')\n",
        "        ax.axhline(y=ANALYSIS_CONFIG['static_threshold'] * 100, color='orange', linestyle='--', alpha=0.5)\n",
        "        ax_idx += 1\n",
        "    \n",
        "    # Coverage\n",
        "    if ANALYSIS_CONFIG['enable_fuzzing'] and ax_idx < len(axes):\n",
        "        ax = axes[ax_idx]\n",
        "        ax.pie([combined_coverage * 100, (1 - combined_coverage) * 100],\n",
        "               labels=['Covered', 'Uncovered'],\n",
        "               colors=['#2ecc71', '#e74c3c'],\n",
        "               autopct='%1.1f%%',\n",
        "               startangle=90)\n",
        "        ax.set_title('Coverage of Changed Lines')\n",
        "        ax_idx += 1\n",
        "    \n",
        "    # Rules\n",
        "    if ANALYSIS_CONFIG['enable_rules'] and ax_idx < len(axes):\n",
        "        ax = axes[ax_idx]\n",
        "        passed_count = len(rules_results) - len(failed_rules)\n",
        "        ax.bar(['Passed', 'Failed'], [passed_count, len(failed_rules)],\n",
        "               color=['#2ecc71', '#e74c3c'], alpha=0.7)\n",
        "        ax.set_ylabel('Count')\n",
        "        ax.set_title(f'Verification Rules ({len(rules_results)} total)')\n",
        "        ax_idx += 1\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('integrated_pipeline_viz.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"‚úì Visualization saved to integrated_pipeline_viz.png\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è  Visualization skipped (need at least 2 modules enabled)\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
