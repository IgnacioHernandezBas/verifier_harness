# Quick Start Guide

## ðŸš€ You're Ready to Go!

All authentication issues are resolved. The system is configured correctly.

## âœ… What's Fixed

1. **Docker Hub authentication** properly configured for Apptainer
2. **Correct SWE-bench image pattern**: `swebench/sweb.eval.x86_64.{repo}_1776_{repo}-{version}:latest`
3. **Images are PUBLIC** - no authentication actually required!
4. **SLURM batch processing** system ready for parallel execution

## ðŸ“‹ Three Ways to Use This

### 1. Interactive Notebook (Development/Testing)
```bash
cd /fs/nexus-scratch/ihbas/verifier_harness
jupyter notebook fuzzing_pipeline_hpc_FIXED.ipynb
```
- Great for: Testing, debugging, exploring single instances
- First container download: 10-15 minutes (then cached)

### 2. SLURM Batch (Recommended for Production)
```bash
# Process 10 instances in parallel
./submit_batch.py --repo "scikit-learn/scikit-learn" --limit 10 --mode analyze --max-parallel 5

# Monitor progress
squeue -u $USER
tail -f logs/analyze_*.out
```
- Great for: Processing many instances efficiently
- See: `BATCH_PROCESSING_README.md` for full details

### 3. Two-Phase Approach (Most Efficient)
```bash
# Phase 1: Build all containers (network-intensive, high parallelism)
./submit_batch.py --limit 20 --mode build --max-parallel 15

# Phase 2: Analyze with cached containers (CPU-intensive, lower parallelism)
./submit_batch.py --limit 20 --mode analyze --max-parallel 5
```
- Great for: Large-scale processing
- Separates I/O-bound from CPU-bound work

## ðŸ“Š Expected Performance

| Phase | Time (first run) | Time (cached) | Resources |
|-------|------------------|---------------|-----------|
| Container Build | 10-15 min | < 1 sec | 8GB RAM, 2 CPU |
| Full Analysis | 2-4 hours | 2-4 hours | 16GB RAM, 4 CPU |

## ðŸŽ¯ Next Steps

1. **Test with one instance** (notebook or single SLURM job)
2. **Build containers** for your target instances
3. **Run full analysis** on batch

## ðŸ’¡ Pro Tips

- Containers are **shared** across all jobs (cached at `/fs/nexus-scratch/ihbas/.cache/swebench_singularity/`)
- Build phase is **network-bound** â†’ use high parallelism (10-20 jobs)
- Analysis phase is **CPU-bound** â†’ use moderate parallelism (3-5 jobs)
- Results saved as JSON in `results/` directory

## ðŸ†˜ Troubleshooting

**Container build timeout?**
â†’ Increase timeout in `slurm_batch_build_containers.sh`: `--time=04:00:00`

**Out of memory?**
â†’ Increase memory: `--mem=32G`

**Authentication errors?**
â†’ Shouldn't happen (images are public), but env vars are set correctly

## ðŸ“ Important Files

```
fuzzing_pipeline_hpc_FIXED.ipynb  â† Notebook (interactive)
submit_batch.py                    â† Submit SLURM jobs
BATCH_PROCESSING_README.md         â† Full batch documentation
slurm_batch_*.sh                   â† SLURM scripts
slurm_worker_*.py                  â† Worker scripts
```

## ðŸŽ‰ You're All Set!

The authentication issue is completely resolved. Start with the notebook to verify everything works, then scale up with SLURM batch processing.
