{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# HPC Fuzzing Pipeline with Dynamic Image Building\n",
    "\n",
    "Complete fuzzing analysis pipeline using dynamic Singularity container building for HPC clusters.\n",
    "\n",
    "## Pipeline Overview:\n",
    "\n",
    "### STATIC ANALYSIS (Host)\n",
    "1. Patch Loading\n",
    "2. Repository Setup  \n",
    "3. Static Analysis (Pylint, Flake8, Radon, Mypy, Bandit)\n",
    "\n",
    "### DYNAMIC ANALYSIS (Container)\n",
    "4. Build Instance-Specific Singularity Container (Dynamic)\n",
    "5. Install Dependencies\n",
    "6. Run Existing Tests\n",
    "7. Patch Analysis\n",
    "8. Generate Hypothesis Tests\n",
    "9. Execute Tests\n",
    "10. Coverage Analysis\n",
    "11. Final Verdict\n",
    "\n",
    "## Key Features:\n",
    "- **Dynamic Image Building**: Automatically builds containers based on SWE-bench instance\n",
    "- **Docker Resolution**: Finds appropriate Docker images from multiple sources\n",
    "- **Caching**: Reuses built images for efficiency\n",
    "- **HPC Optimized**: Designed for cluster environments without Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "from pathlib import Path\n",
    "import json, time, ast\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# Dataset and patch handling\n",
    "from swebench_integration import DatasetLoader, PatchLoader\n",
    "\n",
    "# NEW: Dynamic container building system\n",
    "from swebench_singularity import Config, SingularityBuilder, DockerImageResolver\n",
    "\n",
    "# Existing analysis modules\n",
    "from verifier.dynamic_analyzers.patch_analyzer import PatchAnalyzer\n",
    "from verifier.dynamic_analyzers.test_generator import HypothesisTestGenerator\n",
    "from verifier.dynamic_analyzers.singularity_executor import SingularityTestExecutor\n",
    "from verifier.dynamic_analyzers.coverage_analyzer import CoverageAnalyzer\n",
    "from verifier.dynamic_analyzers.test_patch_singularity import (\n",
    "    install_package_in_singularity, \n",
    "    run_tests_in_singularity\n",
    ")\n",
    "\n",
    "# Static analysis\n",
    "import streamlit.modules.static_eval.static_modules.code_quality as code_quality\n",
    "import streamlit.modules.static_eval.static_modules.syntax_structure as syntax_structure\n",
    "from verifier.utils.diff_utils import parse_unified_diff, filter_paths_to_py\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "print(\"‚úì Imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## Configuration Setup\n",
    "\n",
    "Configure the dynamic container building system for HPC environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize configuration\n",
    "config = Config()\n",
    "\n",
    "# Override defaults for HPC cluster\n",
    "config.set(\"singularity.cache_dir\", \"/fs/nexus-scratch/ihbas/.cache/swebench_singularity\")\n",
    "config.set(\"singularity.tmp_dir\", \"/fs/nexus-scratch/ihbas/.tmp/singularity_build\")\n",
    "config.set(\"singularity.cache_internal_dir\", \"/fs/nexus-scratch/ihbas/.singularity/cache\")\n",
    "config.set(\"singularity.build_timeout\", 1800)  # 30 minutes\n",
    "config.set(\"docker.max_retries\", 3)\n",
    "\n",
    "# Initialize builder\n",
    "builder = SingularityBuilder(config)\n",
    "resolver = DockerImageResolver(config)\n",
    "\n",
    "print(\"‚úì Configuration initialized\")\n",
    "print(f\"  Cache: {config.singularity_cache_dir}\")\n",
    "print(f\"  Tmp: {config.singularity_tmp_dir}\")\n",
    "print(f\"  Singularity available: {builder.check_singularity_available()}\")\n",
    "print(f\"  Docker available: {builder.check_docker_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-header",
   "metadata": {},
   "source": [
    "---\n",
    "# STATIC ANALYSIS\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-patch-header",
   "metadata": {},
   "source": [
    "## Stage 1: Load Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_FILTER = \"scikit-learn/scikit-learn\"  # Example: pytest-dev/pytest, pylint-dev/pylint\n",
    "\n",
    "loader = DatasetLoader(\"princeton-nlp/SWE-bench_Verified\", hf_mode=True, split=\"test\")\n",
    "sample = next(loader.iter_samples(limit=1, filter_repo=REPO_FILTER), None)\n",
    "\n",
    "if sample:\n",
    "    instance_id = sample.get('metadata', {}).get('instance_id', 'unknown')\n",
    "    print(f\"‚úì {instance_id}\")\n",
    "    print(f\"  Repo: {sample['repo']}\")\n",
    "    print(f\"\\nPatch preview:\\n{sample['patch'][:400]}...\")\n",
    "else:\n",
    "    raise Exception(f\"No sample found for {REPO_FILTER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-repo-header",
   "metadata": {},
   "source": [
    "## Stage 2: Setup Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-repo",
   "metadata": {},
   "outputs": [],
   "source": [
    "patcher = PatchLoader(sample=sample, repos_root=\"./repos_temp\")\n",
    "repo_path = patcher.clone_repository()\n",
    "patch_result = patcher.apply_patch()\n",
    "\n",
    "print(f\"‚úì Repo: {repo_path}\")\n",
    "print(f\"‚úì Patch: {'Applied' if patch_result['applied'] else 'FAILED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apply-test-patch-header",
   "metadata": {},
   "source": [
    "## Stage 2b: Apply Test Patch (if exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apply-test-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In SWE-bench, test_patch contains additional tests needed to validate the fix\n",
    "test_patch = sample.get('metadata', {}).get('test_patch', '')\n",
    "\n",
    "if test_patch and test_patch.strip():\n",
    "    print(\"üìù Applying test_patch...\")\n",
    "    try:\n",
    "        test_patch_result = patcher.apply_additional_patch(test_patch)\n",
    "        print(f\"‚úì Test patch applied: {test_patch_result.get('log', 'success')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Test patch application failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No test_patch in metadata (tests already exist in repo)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-analysis-header",
   "metadata": {},
   "source": [
    "## Stage 3: Static Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_config = {\n",
    "    'checks': {'pylint': True, 'flake8': True, 'radon': True, 'mypy': True, 'bandit': True},\n",
    "    'weights': {'pylint': 0.5, 'flake8': 0.15, 'radon': 0.25, 'mypy': 0.05, 'bandit': 0.05}\n",
    "}\n",
    "\n",
    "print(\"üîç Static analysis...\")\n",
    "cq_results = code_quality.analyze(str(repo_path), sample['patch'], static_config)\n",
    "ss_results = syntax_structure.run_syntax_structure_analysis(str(repo_path), sample['patch'])\n",
    "\n",
    "sqi_data = cq_results.get('sqi', {})\n",
    "print(f\"‚úì SQI: {sqi_data.get('SQI', 0)}/100 ({sqi_data.get('classification', 'Unknown')})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-header",
   "metadata": {},
   "source": [
    "---\n",
    "# DYNAMIC ANALYSIS (Container)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "build-container-header",
   "metadata": {},
   "source": [
    "## Stage 4: Build Instance-Specific Container (Dynamic)\n",
    "\n",
    "This is the key improvement: dynamically build a Singularity container based on the SWE-bench instance.\n",
    "The system will:\n",
    "1. Resolve the appropriate Docker image for this instance\n",
    "2. Check the cache for an existing build\n",
    "3. Build the Singularity container if needed\n",
    "4. Cache the result for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-container",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üê≥ Building container for {instance_id}...\\n\")\n",
    "\n",
    "# Check what Docker image will be used\n",
    "docker_image = resolver.find_available_image(instance_id, check_existence=False)\n",
    "if docker_image:\n",
    "    print(f\"  Docker image: {docker_image.full_name}\")\n",
    "    print(f\"  Registry: {docker_image.registry}\")\n",
    "    print(f\"  Tag: {docker_image.tag}\\n\")\n",
    "\n",
    "# Build the container (will use cache if available)\n",
    "build_result = builder.build_instance(\n",
    "    instance_id=instance_id,\n",
    "    force_rebuild=False,  # Set to True to force rebuild\n",
    "    check_docker_exists=False  # Set to True to verify Docker image exists\n",
    ")\n",
    "\n",
    "if build_result.success:\n",
    "    CONTAINER_IMAGE_PATH = build_result.sif_path\n",
    "    cache_status = \"(from cache)\" if build_result.from_cache else \"(newly built)\"\n",
    "    print(f\"‚úì Container ready {cache_status}\")\n",
    "    print(f\"  Path: {CONTAINER_IMAGE_PATH}\")\n",
    "    print(f\"  Build time: {build_result.build_time_seconds:.1f}s\")\n",
    "    \n",
    "    # Verify container works\n",
    "    result = subprocess.run(\n",
    "        [\"singularity\", \"exec\", str(CONTAINER_IMAGE_PATH), \"python\", \"--version\"],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    print(f\"  Python: {result.stdout.strip()}\")\n",
    "else:\n",
    "    print(f\"‚ùå Container build failed: {build_result.error_message}\")\n",
    "    raise Exception(\"Cannot proceed without container\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-deps-header",
   "metadata": {},
   "source": [
    "## Stage 5: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì¶ Installing dependencies...\\n\")\n",
    "\n",
    "install_result = install_package_in_singularity(\n",
    "    repo_path=Path(repo_path),\n",
    "    image_path=str(CONTAINER_IMAGE_PATH)\n",
    ")\n",
    "\n",
    "if install_result.get(\"installed\"):\n",
    "    print(\"‚úì Dependencies installed\")\n",
    "elif install_result.get(\"returncode\") != 0:\n",
    "    print(f\"‚ö†Ô∏è Install issues (code {install_result.get('returncode')})\")\n",
    "    print(install_result.get('stderr', '')[-500:])\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No setup.py/pyproject.toml (will use PYTHONPATH mode)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-tests-header",
   "metadata": {},
   "source": [
    "## Stage 6: Run Existing Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-tests",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß™ Running existing tests...\\n\")\n",
    "\n",
    "# Get tests from metadata\n",
    "fail_to_pass = sample.get('metadata', {}).get('FAIL_TO_PASS', '[]')\n",
    "pass_to_pass = sample.get('metadata', {}).get('PASS_TO_PASS', '[]')\n",
    "\n",
    "print(f\"  FAIL_TO_PASS: {fail_to_pass}\")\n",
    "print(f\"  PASS_TO_PASS: {pass_to_pass}\\n\")\n",
    "\n",
    "# Parse test lists\n",
    "try:\n",
    "    f2p = ast.literal_eval(fail_to_pass) if isinstance(fail_to_pass, str) else fail_to_pass\n",
    "    p2p = ast.literal_eval(pass_to_pass) if isinstance(pass_to_pass, str) else pass_to_pass\n",
    "except:\n",
    "    f2p, p2p = [], []\n",
    "\n",
    "all_tests = f2p + p2p\n",
    "\n",
    "# Run tests using the dynamically built container\n",
    "test_result = run_tests_in_singularity(\n",
    "    repo_path=Path(repo_path),\n",
    "    tests=all_tests,\n",
    "    image_path=str(CONTAINER_IMAGE_PATH)\n",
    ")\n",
    "\n",
    "print(f\"Exit code: {test_result['returncode']}\")\n",
    "print((test_result['stdout'] + test_result['stderr'])[-1500:])\n",
    "print(f\"\\n{'‚úì' if test_result['returncode'] == 0 else '‚ö†Ô∏è'} Tests {'passed' if test_result['returncode'] == 0 else 'had issues'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyze-patch-header",
   "metadata": {},
   "source": [
    "## Stage 7: Analyze Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Analyzing patch...\")\n",
    "\n",
    "patch_analyzer = PatchAnalyzer()\n",
    "modified_files = filter_paths_to_py(list(parse_unified_diff(sample['patch']).keys()))\n",
    "\n",
    "if modified_files:\n",
    "    first_file_path = modified_files[0]\n",
    "    first_file = Path(repo_path) / first_file_path\n",
    "    patched_code = first_file.read_text(encoding='utf-8')\n",
    "    \n",
    "    patch_analysis = patch_analyzer.parse_patch(sample['patch'], patched_code, file_path=first_file_path)\n",
    "    \n",
    "    print(f\"‚úì Files: {len(modified_files)}\")\n",
    "    print(f\"  Module: {patch_analysis.module_path}\")\n",
    "    print(f\"  Functions: {patch_analysis.changed_functions}\")\n",
    "    if patch_analysis.class_context:\n",
    "        print(f\"  Classes: {list(patch_analysis.class_context.values())}\")\n",
    "    print(f\"  Lines: {len(patch_analysis.all_changed_lines)}\")\n",
    "else:\n",
    "    patch_analysis = None\n",
    "    patched_code = None\n",
    "    print(\"‚ö†Ô∏è No Python files modified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generate-tests-header",
   "metadata": {},
   "source": [
    "## Stage 8: Generate Hypothesis Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-tests",
   "metadata": {},
   "outputs": [],
   "source": [
    "if patch_analysis and patch_analysis.changed_functions:\n",
    "    print(\"üß¨ Generating change-aware fuzzing tests...\")\n",
    "    \n",
    "    test_generator = HypothesisTestGenerator()\n",
    "    test_code = test_generator.generate_tests(patch_analysis, patched_code)\n",
    "    test_count = test_code.count('def test_')\n",
    "    \n",
    "    print(f\"‚úì Generated {test_count} tests\")\n",
    "    print(f\"\\nPreview:\\n{test_code[:600]}...\")\n",
    "else:\n",
    "    test_code = None\n",
    "    test_count = 0\n",
    "    print(\"‚ÑπÔ∏è  No testable changes found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "execute-tests-header",
   "metadata": {},
   "source": [
    "## Stage 9: Execute Fuzzing Tests in Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "execute-tests",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_code:\n",
    "    print(\"üê≥ Executing change-aware fuzzing tests in container...\\n\")\n",
    "    \n",
    "    executor = SingularityTestExecutor(str(CONTAINER_IMAGE_PATH), timeout=120)\n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        module_name = patch_analysis.module_path if patch_analysis else None\n",
    "        \n",
    "        success, output, coverage_data = executor.run_tests_with_existing_infrastructure(\n",
    "            Path(repo_path), \n",
    "            test_code,\n",
    "            module_name=module_name\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        print(f\"{'‚úì PASSED' if success else '‚ùå FAILED'} ({elapsed:.1f}s)\\n\")\n",
    "        \n",
    "        # Show output\n",
    "        print(\"=== TEST OUTPUT ===\")\n",
    "        print(output[-2000:])\n",
    "        print(\"=== END OUTPUT ===\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        success = False\n",
    "        coverage_data = {}\n",
    "else:\n",
    "    success = True\n",
    "    coverage_data = {}\n",
    "    print(\"‚ÑπÔ∏è  No tests to execute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coverage-header",
   "metadata": {},
   "source": [
    "## Stage 10: Coverage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "if patch_analysis and coverage_data:\n",
    "    if coverage_data.get('_coverage_skipped'):\n",
    "        print(f\"‚ÑπÔ∏è  Coverage skipped: {coverage_data.get('_skip_reason', 'N/A')}\")\n",
    "        coverage_result = {'overall_coverage': None, 'total_changed_lines': 0, 'total_covered_lines': 0, 'skipped': True}\n",
    "    else:\n",
    "        coverage_analyzer = CoverageAnalyzer()\n",
    "        coverage_result = coverage_analyzer.calculate_changed_line_coverage(\n",
    "            coverage_data, patch_analysis.changed_lines, patch_analysis.all_changed_lines\n",
    "        )\n",
    "        print(f\"üìä Coverage: {coverage_result['overall_coverage']:.1%}\")\n",
    "        print(f\"   {coverage_result['total_covered_lines']}/{coverage_result['total_changed_lines']} lines\")\n",
    "else:\n",
    "    coverage_result = {'overall_coverage': 0.0, 'total_changed_lines': 0, 'total_covered_lines': 0}\n",
    "    print(\"‚ÑπÔ∏è  No coverage data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verdict-header",
   "metadata": {},
   "source": [
    "## Stage 11: Final Verdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verdict",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqi_score = sqi_data.get('SQI', 0) / 100.0\n",
    "coverage_score = coverage_result.get('overall_coverage', 0.0)\n",
    "coverage_skipped = coverage_result.get('skipped', False)\n",
    "\n",
    "# Determine verdict\n",
    "if sqi_score < 0.5:\n",
    "    verdict = 'REJECT'\n",
    "    reason = f'Poor SQI ({sqi_score:.2f})'\n",
    "elif not success:\n",
    "    verdict = 'REJECT'\n",
    "    reason = 'Fuzzing tests failed'\n",
    "elif coverage_skipped:\n",
    "    verdict = 'ACCEPT' if test_count > 0 else 'WARNING'\n",
    "    reason = 'Coverage N/A (internal module)' if test_count > 0 else 'No tests generated'\n",
    "elif coverage_score is not None and coverage_score < 0.5:\n",
    "    verdict = 'WARNING'\n",
    "    reason = f'Low coverage ({coverage_score:.1%})'\n",
    "else:\n",
    "    verdict = 'ACCEPT'\n",
    "    reason = 'All checks passed'\n",
    "\n",
    "# Display verdict\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL VERDICT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{verdict}: {reason}\")\n",
    "print(f\"\\nInstance: {instance_id}\")\n",
    "print(f\"Container: {CONTAINER_IMAGE_PATH.name}\")\n",
    "if coverage_score is not None:\n",
    "    print(f\"\\nSQI: {sqi_score:.2%} | Tests: {test_count} | Coverage: {coverage_score:.1%}\")\n",
    "else:\n",
    "    print(f\"\\nSQI: {sqi_score:.2%} | Tests: {test_count} | Coverage: N/A\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Pipeline Summary\n",
    "\n",
    "This notebook demonstrates the complete fuzzing pipeline with dynamic container building:\n",
    "\n",
    "### Key Improvements:\n",
    "1. **Dynamic Image Building**: Automatically builds instance-specific containers\n",
    "2. **Docker Resolution**: Finds images from multiple registries (aorwall, swebench, ghcr.io)\n",
    "3. **Intelligent Caching**: Reuses containers when possible\n",
    "4. **HPC Optimized**: Works without Docker daemon using Singularity native auth\n",
    "\n",
    "### Next Steps:\n",
    "- Scale to multiple instances using batch processing\n",
    "- Integrate with SLURM for parallel execution\n",
    "- Export results for analysis\n",
    "- Monitor cache usage and performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
