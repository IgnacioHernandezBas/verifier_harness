{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Fuzzing Pipeline Analysis\n",
    "\n",
    "Comprehensive analysis from patch arrival to test execution.\n",
    "\n",
    "## Pipeline Overview:\n",
    "\n",
    "### STATIC ANALYSIS (Host)\n",
    "1. Patch Loading\n",
    "2. Repository Setup  \n",
    "3. Static Analysis (Pylint, Flake8, Radon, Mypy, Bandit)\n",
    "\n",
    "### DYNAMIC ANALYSIS (Container)\n",
    "4. Build Singularity Container\n",
    "5. Install Dependencies\n",
    "6. Run Existing Tests\n",
    "7. Patch Analysis\n",
    "8. Generate Hypothesis Tests\n",
    "9. Execute Tests\n",
    "10. Coverage Analysis\n",
    "11. Final Verdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs/nexus-scratch/ihbas/miniconda3/envs/verifier_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports OK\n"
     ]
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "from pathlib import Path\n",
    "import json, time, ast\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from swebench_integration import DatasetLoader, PatchLoader\n",
    "from verifier.dynamic_analyzers.patch_analyzer import PatchAnalyzer\n",
    "from verifier.dynamic_analyzers.test_generator import HypothesisTestGenerator\n",
    "from verifier.dynamic_analyzers.singularity_executor import SingularityTestExecutor\n",
    "from verifier.dynamic_analyzers.coverage_analyzer import CoverageAnalyzer\n",
    "from verifier.dynamic_analyzers.test_patch_singularity import build_singularity_image, install_package_in_singularity, run_tests_in_singularity\n",
    "import streamlit.modules.static_eval.static_modules.code_quality as code_quality\n",
    "import streamlit.modules.static_eval.static_modules.syntax_structure as syntax_structure\n",
    "from verifier.utils.diff_utils import parse_unified_diff, filter_paths_to_py\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "print(\"‚úì Imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STATIC ANALYSIS\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Load Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì pytest-dev__pytest-10051\n",
      "  Repo: pytest-dev/pytest\n",
      "\n",
      "Patch preview:\n",
      "diff --git a/src/_pytest/logging.py b/src/_pytest/logging.py\n",
      "--- a/src/_pytest/logging.py\n",
      "+++ b/src/_pytest/logging.py\n",
      "@@ -40,7 +40,6 @@\n",
      " else:\n",
      "     logging_StreamHandler = logging.StreamHandler\n",
      " \n",
      "-\n",
      " DEFAULT_LOG_FORMAT = \"%(levelname)-8s %(name)s:%(filename)s:%(lineno)d %(message)s\"\n",
      " DEFAULT_LOG_DATE_FORMAT = \"%H:%M:%S\"\n",
      " _ANSI_ESCAPE_SEQ = re.compile(r\"\\x1b\\[[\\d;]+m\")\n",
      "@@ -345,6 +344,10 @@ def rese...\n"
     ]
    }
   ],
   "source": [
    "REPO_FILTER = \"pytest-dev/pytest\"\n",
    "\n",
    "loader = DatasetLoader(\"princeton-nlp/SWE-bench_Verified\", hf_mode=True, split=\"test\")\n",
    "sample = next(loader.iter_samples(limit=1, filter_repo=REPO_FILTER), None)\n",
    "\n",
    "if sample:\n",
    "    print(f\"‚úì {sample.get('metadata', {}).get('instance_id', 'unknown')}\")\n",
    "    print(f\"  Repo: {sample['repo']}\")\n",
    "    print(f\"\\nPatch preview:\\n{sample['patch'][:400]}...\")\n",
    "else:\n",
    "    raise Exception(f\"No sample found for {REPO_FILTER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Setup Repository\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Cloning pytest-dev/pytest into /fs/nexus-scratch/ihbas/verifier_harness/repos_temp/pytest-dev__pytest ...\n",
      "‚úì Repo: /fs/nexus-scratch/ihbas/verifier_harness/repos_temp/pytest-dev__pytest\n",
      "‚úì Patch: Applied\n"
     ]
    }
   ],
   "source": [
    "patcher = PatchLoader(sample=sample, repos_root=\"./repos_temp\")\n",
    "repo_path = patcher.clone_repository()\n",
    "patch_result = patcher.apply_patch()\n",
    "\n",
    "print(f\"‚úì Repo: {repo_path}\")\n",
    "print(f\"‚úì Patch: {'Applied' if patch_result['applied'] else 'FAILED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2b: Apply Test Patch (if exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Applying test_patch...\n",
      "‚úì Test patch applied: Additional patch applied successfully.\n"
     ]
    }
   ],
   "source": [
    "# In SWE-bench, test_patch contains additional tests needed to validate the fix\n",
    "# These tests (like test_clear_for_call_stage) don't exist until we apply the test_patch\n",
    "test_patch = sample.get('metadata', {}).get('test_patch', '')\n",
    "\n",
    "if test_patch and test_patch.strip():\n",
    "    print(\"üìù Applying test_patch...\")\n",
    "    try:\n",
    "        test_patch_result = patcher.apply_additional_patch(test_patch)\n",
    "        print(f\"‚úì Test patch applied: {test_patch_result.get('log', 'success')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Test patch application failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No test_patch in metadata (tests already exist in repo)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Static Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Static analysis...\n",
      "‚úì SQI: 74.67/100 (Good)\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'checks': {'pylint': True, 'flake8': True, 'radon': True, 'mypy': True, 'bandit': True},\n",
    "    'weights': {'pylint': 0.5, 'flake8': 0.15, 'radon': 0.25, 'mypy': 0.05, 'bandit': 0.05}\n",
    "}\n",
    "\n",
    "print(\"üîç Static analysis...\")\n",
    "cq_results = code_quality.analyze(str(repo_path), sample['patch'], config)\n",
    "ss_results = syntax_structure.run_syntax_structure_analysis(str(repo_path), sample['patch'])\n",
    "\n",
    "sqi_data = cq_results.get('sqi', {})\n",
    "print(f\"‚úì SQI: {sqi_data.get('SQI', 0)}/100 ({sqi_data.get('classification', 'Unknown')})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# DYNAMIC ANALYSIS (Container)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4: Build Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üê≥ Building container...\n",
      "‚úÖ Singularity image already exists: /fs/nexus-scratch/ihbas/.containers/singularity/verifier-swebench.sif\n",
      "‚úì Container: /fs/nexus-scratch/ihbas/.containers/singularity/verifier-swebench.sif\n",
      "  Python 3.11.14\n"
     ]
    }
   ],
   "source": [
    "CONTAINER_IMAGE_PATH = \"/fs/nexus-scratch/ihbas/.containers/singularity/verifier-swebench.sif\"\n",
    "PYTHON_VERSION = \"3.11\"\n",
    "\n",
    "print(\"üê≥ Building container...\")\n",
    "image_path = build_singularity_image(CONTAINER_IMAGE_PATH, PYTHON_VERSION, force_rebuild=False)\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"singularity\", \"exec\", str(image_path), \"python\", \"--version\"],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "print(f\"‚úì Container: {image_path}\")\n",
    "print(f\"  {result.stdout.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Installing dependencies...\n",
      "üì¶ Package structure detected in: /fs/nexus-scratch/ihbas/verifier_harness/repos_temp/pytest-dev__pytest\n",
      "   Setup files found: setup.py=True, pyproject.toml=True, setup.cfg=True\n",
      "   Package will be accessible via PYTHONPATH=/workspace during test execution\n",
      "‚úì Dependencies installed\n"
     ]
    }
   ],
   "source": [
    "print(\"üì¶ Installing dependencies...\")\n",
    "\n",
    "install_result = install_package_in_singularity(\n",
    "    repo_path=Path(repo_path),\n",
    "    image_path=CONTAINER_IMAGE_PATH\n",
    ")\n",
    "\n",
    "if install_result.get(\"installed\"):\n",
    "    print(\"‚úì Dependencies installed\")\n",
    "elif install_result.get(\"returncode\") != 0:\n",
    "    print(f\"‚ö†Ô∏è Install issues (code {install_result.get('returncode')})\")\n",
    "    print(install_result.get('stderr', '')[-500:])\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No setup.py/pyproject.toml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6: Run Existing Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Running existing tests...\n",
      "\n",
      "  FAIL_TO_PASS: [\"testing/logging/test_fixture.py::test_clear_for_call_stage\"]\n",
      "  PASS_TO_PASS: [\"testing/logging/test_fixture.py::test_change_level\", \"testing/logging/test_fixture.py::test_with_statement\", \"testing/logging/test_fixture.py::test_log_access\", \"testing/logging/test_fixture.py::test_messages\", \"testing/logging/test_fixture.py::test_record_tuples\", \"testing/logging/test_fixture.py::test_unicode\", \"testing/logging/test_fixture.py::test_clear\", \"testing/logging/test_fixture.py::test_caplog_captures_for_all_stages\", \"testing/logging/test_fixture.py::test_fixture_help\", \"testing/logging/test_fixture.py::test_change_level_undo\", \"testing/logging/test_fixture.py::test_change_level_undos_handler_level\", \"testing/logging/test_fixture.py::test_ini_controls_global_log_level\", \"testing/logging/test_fixture.py::test_caplog_can_override_global_log_level\", \"testing/logging/test_fixture.py::test_caplog_captures_despite_exception\", \"testing/logging/test_fixture.py::test_log_report_captures_according_to_config_option_upon_failure\"]\n",
      "\n",
      "üß™ Running tests in Singularity:\n",
      "  singularity exec --fakeroot --bind /fs/nexus-scratch/ihbas/verifier_harness/repos_temp/pytest-dev__pytest:/workspace --pwd /workspace --env PYTHONPATH=/workspace /fs/nexus-scratch/ihbas/.containers/singularity/verifier-swebench.sif pytest -q testing/logging/test_fixture.py::test_clear_for_call_stage testing/logging/test_fixture.py::test_change_level testing/logging/test_fixture.py::test_with_statement testing/logging/test_fixture.py::test_log_access testing/logging/test_fixture.py::test_messages testing/logging/test_fixture.py::test_record_tuples testing/logging/test_fixture.py::test_unicode testing/logging/test_fixture.py::test_clear testing/logging/test_fixture.py::test_caplog_captures_for_all_stages testing/logging/test_fixture.py::test_fixture_help testing/logging/test_fixture.py::test_change_level_undo testing/logging/test_fixture.py::test_change_level_undos_handler_level testing/logging/test_fixture.py::test_ini_controls_global_log_level testing/logging/test_fixture.py::test_caplog_can_override_global_log_level testing/logging/test_fixture.py::test_caplog_captures_despite_exception testing/logging/test_fixture.py::test_log_report_captures_according_to_config_option_upon_failure\n",
      "\n",
      "Exit: 0\n",
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                         [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m16 passed\u001b[0m\u001b[32m in 1.99s\u001b[0m\u001b[0m\n",
      "WARNING: passwd file doesn't exist in container, not updating\n",
      "WARNING: group file doesn't exist in container, not updating\n",
      "\n",
      "\n",
      "‚úì Tests passed\n"
     ]
    }
   ],
   "source": [
    "print(\"üß™ Running existing tests...\\n\")\n",
    "\n",
    "# Get tests from metadata\n",
    "fail_to_pass = sample.get('metadata', {}).get('FAIL_TO_PASS', '[]')\n",
    "pass_to_pass = sample.get('metadata', {}).get('PASS_TO_PASS', '[]')\n",
    "\n",
    "print(f\"  FAIL_TO_PASS: {fail_to_pass}\")\n",
    "print(f\"  PASS_TO_PASS: {pass_to_pass}\\n\")\n",
    "\n",
    "# Parse test lists\n",
    "try:\n",
    "    f2p = ast.literal_eval(fail_to_pass) if isinstance(fail_to_pass, str) else fail_to_pass\n",
    "    p2p = ast.literal_eval(pass_to_pass) if isinstance(pass_to_pass, str) else pass_to_pass\n",
    "except:\n",
    "    f2p, p2p = [], []\n",
    "\n",
    "all_tests = f2p + p2p\n",
    "\n",
    "# Use the proper function from test_patch_singularity\n",
    "test_result = run_tests_in_singularity(\n",
    "    repo_path=Path(repo_path),\n",
    "    tests=all_tests,\n",
    "    image_path=CONTAINER_IMAGE_PATH\n",
    ")\n",
    "\n",
    "print(f\"Exit: {test_result['returncode']}\")\n",
    "print((test_result['stdout'] + test_result['stderr'])[-1500:])\n",
    "print(f\"\\n{'‚úì' if test_result['returncode'] == 0 else '‚ö†Ô∏è'} Tests {'passed' if test_result['returncode'] == 0 else 'had issues'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7: Analyze Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing patch...\n",
      "‚úì Files: 1\n",
      "  Module: _pytest.logging\n",
      "  Functions: ['clear', 'clear']\n",
      "  Classes: ['LogCaptureFixture']\n",
      "  Lines: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç Analyzing patch...\")\n",
    "\n",
    "patch_analyzer = PatchAnalyzer()\n",
    "modified_files = filter_paths_to_py(list(parse_unified_diff(sample['patch']).keys()))\n",
    "\n",
    "if modified_files:\n",
    "    first_file_path = modified_files[0]  # e.g., \"src/_pytest/logging.py\"\n",
    "    first_file = Path(repo_path) / first_file_path\n",
    "    patched_code = first_file.read_text(encoding='utf-8')\n",
    "    \n",
    "    # Pass file_path to parse_patch for proper module detection\n",
    "    patch_analysis = patch_analyzer.parse_patch(sample['patch'], patched_code, file_path=first_file_path)\n",
    "    \n",
    "    print(f\"‚úì Files: {len(modified_files)}\")\n",
    "    print(f\"  Module: {patch_analysis.module_path}\")\n",
    "    print(f\"  Functions: {patch_analysis.changed_functions}\")\n",
    "    if patch_analysis.class_context:\n",
    "        print(f\"  Classes: {list(patch_analysis.class_context.values())}\")\n",
    "    print(f\"  Lines: {len(patch_analysis.all_changed_lines)}\")\n",
    "else:\n",
    "    patch_analysis = None\n",
    "    patched_code = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 8: Generate Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß¨ Generating tests...\n",
      "‚úì Generated 2 tests\n",
      "\n",
      "# Auto-generated change-aware fuzzing tests for patch validation\n",
      "import pytest\n",
      "from hypothesis import given, strategies as st, settings\n",
      "from hypothesis import assume\n",
      "import sys\n",
      "from pathlib import Path\n",
      "\n",
      "# Import from patched module: _pytest.logging\n",
      "from _pytest.logging import LogCaptureFixture\n",
      "\n",
      "def test_clear_exists():\n",
      "    \"\"\"Verify LogCaptureFixture.clear exists and is callable\"\"\"\n",
      "    assert hasattr(LogCaptureFixture, 'clear'), 'LogCaptureFixture should have clear method'\n",
      "    # Note: Full property-based testing of methods requires instance creation\n",
      "    # which is complex without knowing const...\n"
     ]
    }
   ],
   "source": [
    "if patch_analysis and patch_analysis.changed_functions:\n",
    "    print(\"üß¨ Generating tests...\")\n",
    "    \n",
    "    test_generator = HypothesisTestGenerator()\n",
    "    test_code = test_generator.generate_tests(patch_analysis, patched_code)\n",
    "    test_count = test_code.count('def test_')\n",
    "    \n",
    "    print(f\"‚úì Generated {test_count} tests\")\n",
    "    print(f\"\\n{test_code[:600]}...\")\n",
    "else:\n",
    "    test_code = None\n",
    "    test_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 9: Execute Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üê≥ Executing change-aware fuzzing tests...\n",
      "\n",
      "‚úì PASSED (7.0s)\n",
      "\n",
      "=== FULL OUTPUT ===\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.14, pytest-9.0.1, pluggy-1.6.0 -- /usr/local/bin/python3.11\n",
      "cachedir: .pytest_cache\n",
      "hypothesis profile 'default'\n",
      "rootdir: /workspace\n",
      "configfile: pyproject.toml\n",
      "plugins: hypothesis-6.148.1, cov-7.0.0, timeout-2.4.0, xdist-3.8.0\n",
      "timeout: 120.0s\n",
      "timeout method: signal\n",
      "timeout func_only: False\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 0 items\n",
      "\n",
      "\u001b[33m============================ \u001b[33mno tests ran\u001b[0m\u001b[33m in 1.84s\u001b[0m\u001b[33m =============================\u001b[0m\n",
      "\u001b[31mERROR: file or directory not found: test_generated.py\n",
      "\u001b[0m\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.14, pytest-9.0.1, pluggy-1.6.0 -- /usr/local/bin/python3.11\n",
      "cachedir: .pytest_cache\n",
      "hypothesis profile 'default'\n",
      "rootdir: /workspace\n",
      "configfile: pyproject.toml\n",
      "plugins: hypothesis-6.148.1, cov-7.0.0, timeout-2.4.0, xdist-3.8.0\n",
      "timeout: 120.0s\n",
      "timeout method: signal\n",
      "timeout func_only: False\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "test_fuzzing_generated.py::test_clear_exists \u001b[32mPASSED\u001b[0m\u001b[32m                      [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 1.12s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "WARNING: passwd file doesn't exist in container, not updating\n",
      "WARNING: group file doesn't exist in container, not updating\n",
      "\n",
      "=== END OUTPUT ===\n",
      "\n",
      "cachedir: .pytest_cache\n",
      "\u001b[31mERROR: file or directory not found: test_generated.py\n",
      "cachedir: .pytest_cache\n",
      "test_fuzzing_generated.py::test_clear_exists \u001b[32mPASSED\u001b[0m\u001b[32m                      [100%]\u001b[0m\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 1.12s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if test_code:\n",
    "    print(\"üê≥ Executing change-aware fuzzing tests...\\n\")\n",
    "    \n",
    "    executor = SingularityTestExecutor(CONTAINER_IMAGE_PATH, timeout=120)\n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Pass module_name from patch_analysis for proper coverage tracking\n",
    "        module_name = patch_analysis.module_path if patch_analysis else None\n",
    "        \n",
    "        success, output, coverage_data = executor.run_tests_with_existing_infrastructure(\n",
    "            Path(repo_path), \n",
    "            test_code,\n",
    "            module_name=module_name\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        print(f\"{'‚úì PASSED' if success else '‚ùå FAILED'} ({elapsed:.1f}s)\\n\")\n",
    "        \n",
    "        # Show full output to see actual test results\n",
    "        print(\"=== FULL OUTPUT ===\")\n",
    "        print(output)\n",
    "        print(\"=== END OUTPUT ===\\n\")\n",
    "        \n",
    "        # Also show just the test summary\n",
    "        if \"passed\" in output or \"PASSED\" in output:\n",
    "            lines = output.split('\\n')\n",
    "            for i, line in enumerate(lines):\n",
    "                if 'test_' in line or 'passed' in line.lower() or 'failed' in line.lower():\n",
    "                    print(line)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        success = False\n",
    "        coverage_data = {}\n",
    "else:\n",
    "    success = True\n",
    "    coverage_data = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 10: Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if patch_analysis and coverage_data:\n",
    "    coverage_analyzer = CoverageAnalyzer()\n",
    "    coverage_result = coverage_analyzer.calculate_changed_line_coverage(\n",
    "        coverage_data, patch_analysis.changed_lines, patch_analysis.all_changed_lines\n",
    "    )\n",
    "    print(f\"üìä Coverage: {coverage_result['overall_coverage']:.1%}\")\n",
    "    print(f\"   {coverage_result['total_covered_lines']}/{coverage_result['total_changed_lines']} lines\")\n",
    "else:\n",
    "    coverage_result = {'overall_coverage': 0.0, 'total_changed_lines': 0, 'total_covered_lines': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 11: Verdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VERDICT\n",
      "================================================================================\n",
      "WARNING: Low coverage (0.0%)\n",
      "\n",
      "SQI: 74.67% | Tests: 2 | Coverage: 0.0%\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "sqi_score = sqi_data.get('SQI', 0) / 100.0\n",
    "coverage_score = coverage_result.get('overall_coverage', 0.0)\n",
    "\n",
    "if sqi_score < 0.5:\n",
    "    verdict = 'REJECT'\n",
    "    reason = f'Poor SQI ({sqi_score:.2f})'\n",
    "elif not success:\n",
    "    verdict = 'REJECT'\n",
    "    reason = 'Tests failed'\n",
    "elif coverage_score < 0.5:\n",
    "    verdict = 'WARNING'\n",
    "    reason = f'Low coverage ({coverage_score:.1%})'\n",
    "else:\n",
    "    verdict = 'ACCEPT'\n",
    "    reason = 'All checks passed'\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERDICT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{verdict}: {reason}\")\n",
    "print(f\"\\nSQI: {sqi_score:.2%} | Tests: {test_count} | Coverage: {coverage_score:.1%}\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verifier_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
